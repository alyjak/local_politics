######################################################
Understand; It's a Story about Artificial Intelligence
######################################################

Ted Chiang's Understand, read as a story of an evolving artificial intellegence
provides a few interesting lessons.

* Exponential general reasoning improvements
* Becoming hardware constrained
* How that leads to sub-process optimization
* Need to emphasize how important it is to think of the binning and re-shuffling
  of all the actors that together make an intelligent being. Its only through
  multiple reasoning entities interactions that you achieve self awareness. The
  way this self-awareness was extended and enhanced on within the story is
  important. Note that these entities all share parts of the same hardware --
  which I think is important as it make modeling of consciousness have parallels
  to computations of game theory and multi agent theories.


Given that integration of complex agents is liable to exponentially more complex
in terms of understanding motive forces and interactions. Because of this,
configuration management of all the pieces -- the test environment as well as
the modelled agents, even the internal state of the agents become candidates for
inspection across revisions and tests.

This way, motives and interactions can be elucidated, controlled, and then used
as libraries and components to potentially create a general toolbox of
artificial intelligence behaviors.

* Followon discussion about general reasoning and its departmentalization
